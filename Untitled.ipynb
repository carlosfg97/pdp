{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package xgboost:\n",
      "\n",
      "NAME\n",
      "    xgboost - XGBoost: eXtreme Gradient Boosting library.\n",
      "\n",
      "DESCRIPTION\n",
      "    Contributors: https://github.com/dmlc/xgboost/blob/master/CONTRIBUTORS.md\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    callback\n",
      "    compat\n",
      "    core\n",
      "    libpath\n",
      "    plotting\n",
      "    rabit\n",
      "    sklearn\n",
      "    training\n",
      "\n",
      "CLASSES\n",
      "    builtins.object\n",
      "        xgboost.core.Booster\n",
      "        xgboost.core.DMatrix\n",
      "    sklearn.base.BaseEstimator(builtins.object)\n",
      "        xgboost.sklearn.XGBModel\n",
      "            xgboost.sklearn.XGBClassifier(xgboost.sklearn.XGBModel, sklearn.base.ClassifierMixin)\n",
      "            xgboost.sklearn.XGBRegressor(xgboost.sklearn.XGBModel, sklearn.base.RegressorMixin)\n",
      "    \n",
      "    class Booster(builtins.object)\n",
      "     |  \"A Booster of of XGBoost.\n",
      "     |  \n",
      "     |  Booster is the model of xgboost, that contains low level routines for\n",
      "     |  training, prediction and evaluation.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __copy__(self)\n",
      "     |  \n",
      "     |  __deepcopy__(self, _)\n",
      "     |  \n",
      "     |  __del__(self)\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __init__(self, params=None, cache=(), model_file=None)\n",
      "     |      Initialize the Booster.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      params : dict\n",
      "     |          Parameters for boosters.\n",
      "     |      cache : list\n",
      "     |          List of cache items.\n",
      "     |      model_file : string\n",
      "     |          Path to the model file.\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  attr(self, key)\n",
      "     |      Get attribute string from the Booster.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      key : str\n",
      "     |          The key to get attribute from.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      value : str\n",
      "     |          The attribute value of the key, returns None if attribute do not exist.\n",
      "     |  \n",
      "     |  attributes(self)\n",
      "     |      Get attributes stored in the Booster as a dictionary.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      result : dictionary of  attribute_name: attribute_value pairs of strings.\n",
      "     |          Returns an empty dict if there's no attributes.\n",
      "     |  \n",
      "     |  boost(self, dtrain, grad, hess)\n",
      "     |      Boost the booster for one iteration, with customized gradient statistics.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      dtrain : DMatrix\n",
      "     |          The training DMatrix.\n",
      "     |      grad : list\n",
      "     |          The first order of gradient.\n",
      "     |      hess : list\n",
      "     |          The second order of gradient.\n",
      "     |  \n",
      "     |  copy(self)\n",
      "     |      Copy the booster object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      booster: `Booster`\n",
      "     |          a copied booster model\n",
      "     |  \n",
      "     |  dump_model(self, fout, fmap='', with_stats=False)\n",
      "     |      Dump model into a text file.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      foout : string\n",
      "     |          Output file name.\n",
      "     |      fmap : string, optional\n",
      "     |          Name of the file containing feature map names.\n",
      "     |      with_stats : bool (optional)\n",
      "     |          Controls whether the split statistics are output.\n",
      "     |  \n",
      "     |  eval(self, data, name='eval', iteration=0)\n",
      "     |      Evaluate the model on mat.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      data : DMatrix\n",
      "     |          The dmatrix storing the input.\n",
      "     |      \n",
      "     |      name : str, optional\n",
      "     |          The name of the dataset.\n",
      "     |      \n",
      "     |      iteration : int, optional\n",
      "     |          The current iteration number.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      result: str\n",
      "     |          Evaluation result string.\n",
      "     |  \n",
      "     |  eval_set(self, evals, iteration=0, feval=None)\n",
      "     |      Evaluate a set of data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      evals : list of tuples (DMatrix, string)\n",
      "     |          List of items to be evaluated.\n",
      "     |      iteration : int\n",
      "     |          Current iteration.\n",
      "     |      feval : function\n",
      "     |          Custom evaluation function.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      result: str\n",
      "     |          Evaluation result string.\n",
      "     |  \n",
      "     |  get_dump(self, fmap='', with_stats=False, dump_format='text')\n",
      "     |      Returns the dump the model as a list of strings.\n",
      "     |  \n",
      "     |  get_fscore(self, fmap='')\n",
      "     |      Get feature importance of each feature.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fmap: str (optional)\n",
      "     |         The name of feature map file\n",
      "     |  \n",
      "     |  get_score(self, fmap='', importance_type='weight')\n",
      "     |      Get feature importance of each feature.\n",
      "     |      Importance type can be defined as:\n",
      "     |          'weight' - the number of times a feature is used to split the data across all trees.\n",
      "     |          'gain' - the average gain of the feature when it is used in trees\n",
      "     |          'cover' - the average coverage of the feature when it is used in trees\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fmap: str (optional)\n",
      "     |         The name of feature map file\n",
      "     |  \n",
      "     |  get_split_value_histogram(self, feature, fmap='', bins=None, as_pandas=True)\n",
      "     |      Get split value histogram of a feature\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      feature: str\n",
      "     |          The name of the feature.\n",
      "     |      fmap: str (optional)\n",
      "     |          The name of feature map file.\n",
      "     |      bin: int, default None\n",
      "     |          The maximum number of bins.\n",
      "     |          Number of bins equals number of unique split values n_unique,\n",
      "     |          if bins == None or bins > n_unique.\n",
      "     |      as_pandas : bool, default True\n",
      "     |          Return pd.DataFrame when pandas is installed.\n",
      "     |          If False or pandas is not installed, return numpy ndarray.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      a histogram of used splitting values for the specified feature\n",
      "     |      either as numpy array or pandas DataFrame.\n",
      "     |  \n",
      "     |  load_model(self, fname)\n",
      "     |      Load the model from a file.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname : string or a memory buffer\n",
      "     |          Input file name or memory buffer(see also save_raw)\n",
      "     |  \n",
      "     |  load_rabit_checkpoint(self)\n",
      "     |      Initialize the model by load from rabit checkpoint.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      version: integer\n",
      "     |          The version number of the model.\n",
      "     |  \n",
      "     |  predict(self, data, output_margin=False, ntree_limit=0, pred_leaf=False, pred_contribs=False, approx_contribs=False)\n",
      "     |      Predict with data.\n",
      "     |      \n",
      "     |      NOTE: This function is not thread safe.\n",
      "     |            For each booster object, predict can only be called from one thread.\n",
      "     |            If you want to run prediction using multiple thread, call bst.copy() to make copies\n",
      "     |            of model object and then call predict\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      data : DMatrix\n",
      "     |          The dmatrix storing the input.\n",
      "     |      \n",
      "     |      output_margin : bool\n",
      "     |          Whether to output the raw untransformed margin value.\n",
      "     |      \n",
      "     |      ntree_limit : int\n",
      "     |          Limit number of trees in the prediction; defaults to 0 (use all trees).\n",
      "     |      \n",
      "     |      pred_leaf : bool\n",
      "     |          When this option is on, the output will be a matrix of (nsample, ntrees)\n",
      "     |          with each record indicating the predicted leaf index of each sample in each tree.\n",
      "     |          Note that the leaf index of a tree is unique per tree, so you may find leaf 1\n",
      "     |          in both tree 1 and tree 0.\n",
      "     |      \n",
      "     |      pred_contribs : bool\n",
      "     |          When this option is on, the output will be a matrix of (nsample, nfeats+1)\n",
      "     |          with each record indicating the feature contributions (SHAP values) for that\n",
      "     |          prediction. The sum of all feature contributions is equal to the prediction.\n",
      "     |          Note that the bias is added as the final column, on top of the regular features.\n",
      "     |      \n",
      "     |      approx_contribs : bool\n",
      "     |          Approximate the contributions of each feature\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      prediction : numpy array\n",
      "     |  \n",
      "     |  save_model(self, fname)\n",
      "     |      Save the model to a file.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname : string\n",
      "     |          Output file name\n",
      "     |  \n",
      "     |  save_rabit_checkpoint(self)\n",
      "     |      Save the current booster to rabit checkpoint.\n",
      "     |  \n",
      "     |  save_raw(self)\n",
      "     |      Save the model to a in memory buffer representation\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      a in memory buffer representation of the model\n",
      "     |  \n",
      "     |  set_attr(self, **kwargs)\n",
      "     |      Set the attribute of the Booster.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **kwargs\n",
      "     |          The attributes to set. Setting a value to None deletes an attribute.\n",
      "     |  \n",
      "     |  set_param(self, params, value=None)\n",
      "     |      Set parameters into the Booster.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      params: dict/list/str\n",
      "     |         list of key,value pairs, dict of key to value or simply str key\n",
      "     |      value: optional\n",
      "     |         value of the specified parameter, when params is str key\n",
      "     |  \n",
      "     |  update(self, dtrain, iteration, fobj=None)\n",
      "     |      Update for one iteration, with objective function calculated internally.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      dtrain : DMatrix\n",
      "     |          Training data.\n",
      "     |      iteration : int\n",
      "     |          Current iteration number.\n",
      "     |      fobj : function\n",
      "     |          Customized objective function.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  feature_names = None\n",
      "    \n",
      "    class DMatrix(builtins.object)\n",
      "     |  Data Matrix used in XGBoost.\n",
      "     |  \n",
      "     |  DMatrix is a internal data structure that used by XGBoost\n",
      "     |  which is optimized for both memory efficiency and training speed.\n",
      "     |  You can construct DMatrix from numpy.arrays\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __del__(self)\n",
      "     |  \n",
      "     |  __init__(self, data, label=None, missing=None, weight=None, silent=False, feature_names=None, feature_types=None, nthread=None)\n",
      "     |      Data matrix used in XGBoost.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      data : string/numpy array/scipy.sparse/pd.DataFrame\n",
      "     |          Data source of DMatrix.\n",
      "     |          When data is string type, it represents the path libsvm format txt file,\n",
      "     |          or binary file that xgboost can read from.\n",
      "     |      label : list or numpy 1-D array, optional\n",
      "     |          Label of the training data.\n",
      "     |      missing : float, optional\n",
      "     |          Value in the data which needs to be present as a missing value. If\n",
      "     |          None, defaults to np.nan.\n",
      "     |      weight : list or numpy 1-D array , optional\n",
      "     |          Weight for each instance.\n",
      "     |      silent : boolean, optional\n",
      "     |          Whether print messages during construction\n",
      "     |      feature_names : list, optional\n",
      "     |          Set names for features.\n",
      "     |      feature_types : list, optional\n",
      "     |          Set types for features.\n",
      "     |  \n",
      "     |  get_base_margin(self)\n",
      "     |      Get the base margin of the DMatrix.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      base_margin : float\n",
      "     |  \n",
      "     |  get_float_info(self, field)\n",
      "     |      Get float property from the DMatrix.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      field: str\n",
      "     |          The field name of the information\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      info : array\n",
      "     |          a numpy array of float information of the data\n",
      "     |  \n",
      "     |  get_label(self)\n",
      "     |      Get the label of the DMatrix.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      label : array\n",
      "     |  \n",
      "     |  get_uint_info(self, field)\n",
      "     |      Get unsigned integer property from the DMatrix.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      field: str\n",
      "     |          The field name of the information\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      info : array\n",
      "     |          a numpy array of float information of the data\n",
      "     |  \n",
      "     |  get_weight(self)\n",
      "     |      Get the weight of the DMatrix.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      weight : array\n",
      "     |  \n",
      "     |  num_col(self)\n",
      "     |      Get the number of columns (features) in the DMatrix.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      number of columns : int\n",
      "     |  \n",
      "     |  num_row(self)\n",
      "     |      Get the number of rows in the DMatrix.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      number of rows : int\n",
      "     |  \n",
      "     |  save_binary(self, fname, silent=True)\n",
      "     |      Save DMatrix to an XGBoost buffer.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname : string\n",
      "     |          Name of the output buffer file.\n",
      "     |      silent : bool (optional; default: True)\n",
      "     |          If set, the output is suppressed.\n",
      "     |  \n",
      "     |  set_base_margin(self, margin)\n",
      "     |      Set base margin of booster to start from.\n",
      "     |      \n",
      "     |      This can be used to specify a prediction value of\n",
      "     |      existing model to be base_margin\n",
      "     |      However, remember margin is needed, instead of transformed prediction\n",
      "     |      e.g. for logistic regression: need to put in value before logistic transformation\n",
      "     |      see also example/demo.py\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      margin: array like\n",
      "     |          Prediction margin of each datapoint\n",
      "     |  \n",
      "     |  set_float_info(self, field, data)\n",
      "     |      Set float type property into the DMatrix.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      field: str\n",
      "     |          The field name of the information\n",
      "     |      \n",
      "     |      data: numpy array\n",
      "     |          The array of data to be set\n",
      "     |  \n",
      "     |  set_float_info_npy2d(self, field, data)\n",
      "     |      Set float type property into the DMatrix\n",
      "     |         for numpy 2d array input\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      field: str\n",
      "     |          The field name of the information\n",
      "     |      \n",
      "     |      data: numpy array\n",
      "     |          The array of data to be set\n",
      "     |  \n",
      "     |  set_group(self, group)\n",
      "     |      Set group size of DMatrix (used for ranking).\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      group : array like\n",
      "     |          Group size of each group\n",
      "     |  \n",
      "     |  set_label(self, label)\n",
      "     |      Set label of dmatrix\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      label: array like\n",
      "     |          The label information to be set into DMatrix\n",
      "     |  \n",
      "     |  set_label_npy2d(self, label)\n",
      "     |      Set label of dmatrix\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      label: array like\n",
      "     |          The label information to be set into DMatrix\n",
      "     |          from numpy 2D array\n",
      "     |  \n",
      "     |  set_uint_info(self, field, data)\n",
      "     |      Set uint type property into the DMatrix.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      field: str\n",
      "     |          The field name of the information\n",
      "     |      \n",
      "     |      data: numpy array\n",
      "     |          The array of data to be set\n",
      "     |  \n",
      "     |  set_weight(self, weight)\n",
      "     |      Set weight of each instance.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      weight : array like\n",
      "     |          Weight for each data point\n",
      "     |  \n",
      "     |  set_weight_npy2d(self, weight)\n",
      "     |      Set weight of each instance\n",
      "     |          for numpy 2D array\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      weight : array like\n",
      "     |          Weight for each data point in numpy 2D array\n",
      "     |  \n",
      "     |  slice(self, rindex)\n",
      "     |      Slice the DMatrix and return a new DMatrix that only contains `rindex`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      rindex : list\n",
      "     |          List of indices to be selected.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      res : DMatrix\n",
      "     |          A new DMatrix containing only selected indices.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  feature_names\n",
      "     |      Get feature names (column labels).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      feature_names : list or None\n",
      "     |  \n",
      "     |  feature_types\n",
      "     |      Get feature types (column types).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      feature_types : list or None\n",
      "    \n",
      "    class XGBClassifier(XGBModel, sklearn.base.ClassifierMixin)\n",
      "     |  Implementation of the scikit-learn API for XGBoost classification.\n",
      "     |  \n",
      "     |      Parameters\n",
      "     |  ----------\n",
      "     |  max_depth : int\n",
      "     |      Maximum tree depth for base learners.\n",
      "     |  learning_rate : float\n",
      "     |      Boosting learning rate (xgb's \"eta\")\n",
      "     |  n_estimators : int\n",
      "     |      Number of boosted trees to fit.\n",
      "     |  silent : boolean\n",
      "     |      Whether to print messages while running boosting.\n",
      "     |  objective : string or callable\n",
      "     |      Specify the learning task and the corresponding learning objective or\n",
      "     |      a custom objective function to be used (see note below).\n",
      "     |  booster: string\n",
      "     |      Specify which booster to use: gbtree, gblinear or dart.\n",
      "     |  nthread : int\n",
      "     |      Number of parallel threads used to run xgboost.  (Deprecated, please use n_jobs)\n",
      "     |  n_jobs : int\n",
      "     |      Number of parallel threads used to run xgboost.  (replaces nthread)\n",
      "     |  gamma : float\n",
      "     |      Minimum loss reduction required to make a further partition on a leaf node of the tree.\n",
      "     |  min_child_weight : int\n",
      "     |      Minimum sum of instance weight(hessian) needed in a child.\n",
      "     |  max_delta_step : int\n",
      "     |      Maximum delta step we allow each tree's weight estimation to be.\n",
      "     |  subsample : float\n",
      "     |      Subsample ratio of the training instance.\n",
      "     |  colsample_bytree : float\n",
      "     |      Subsample ratio of columns when constructing each tree.\n",
      "     |  colsample_bylevel : float\n",
      "     |      Subsample ratio of columns for each split, in each level.\n",
      "     |  reg_alpha : float (xgb's alpha)\n",
      "     |      L1 regularization term on weights\n",
      "     |  reg_lambda : float (xgb's lambda)\n",
      "     |      L2 regularization term on weights\n",
      "     |  scale_pos_weight : float\n",
      "     |      Balancing of positive and negative weights.\n",
      "     |  base_score:\n",
      "     |      The initial prediction score of all instances, global bias.\n",
      "     |  seed : int\n",
      "     |      Random number seed.  (Deprecated, please use random_state)\n",
      "     |  random_state : int\n",
      "     |      Random number seed.  (replaces seed)\n",
      "     |  missing : float, optional\n",
      "     |      Value in the data which needs to be present as a missing value. If\n",
      "     |      None, defaults to np.nan.\n",
      "     |  **kwargs : dict, optional\n",
      "     |      Keyword arguments for XGBoost Booster object.  Full documentation of parameters can\n",
      "     |      be found here: https://github.com/dmlc/xgboost/blob/master/doc/parameter.md.\n",
      "     |      Attempting to set a parameter via the constructor args and **kwargs dict simultaneously\n",
      "     |      will result in a TypeError.\n",
      "     |      Note:\n",
      "     |          **kwargs is unsupported by Sklearn.  We do not guarantee that parameters passed via\n",
      "     |          this argument will interact properly with Sklearn.\n",
      "     |  \n",
      "     |  Note\n",
      "     |  ----\n",
      "     |  A custom objective function can be provided for the ``objective``\n",
      "     |  parameter. In this case, it should have the signature\n",
      "     |  ``objective(y_true, y_pred) -> grad, hess``:\n",
      "     |  \n",
      "     |  y_true: array_like of shape [n_samples]\n",
      "     |      The target values\n",
      "     |  y_pred: array_like of shape [n_samples]\n",
      "     |      The predicted values\n",
      "     |  \n",
      "     |  grad: array_like of shape [n_samples]\n",
      "     |      The value of the gradient for each sample point.\n",
      "     |  hess: array_like of shape [n_samples]\n",
      "     |      The value of the second derivative for each sample point\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      XGBClassifier\n",
      "     |      XGBModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.ClassifierMixin\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, max_depth=3, learning_rate=0.1, n_estimators=100, silent=True, objective='binary:logistic', booster='gbtree', n_jobs=1, nthread=None, gamma=0, min_child_weight=1, max_delta_step=0, subsample=1, colsample_bytree=1, colsample_bylevel=1, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, base_score=0.5, random_state=0, seed=None, missing=None, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  evals_result(self)\n",
      "     |      Return the evaluation results.\n",
      "     |      \n",
      "     |      If eval_set is passed to the `fit` function, you can call evals_result() to\n",
      "     |      get evaluation results for all passed eval_sets. When eval_metric is also\n",
      "     |      passed to the `fit` function, the evals_result will contain the eval_metrics\n",
      "     |      passed to the `fit` function\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      evals_result : dictionary\n",
      "     |      \n",
      "     |      Example\n",
      "     |      -------\n",
      "     |      param_dist = {'objective':'binary:logistic', 'n_estimators':2}\n",
      "     |      \n",
      "     |      clf = xgb.XGBClassifier(**param_dist)\n",
      "     |      \n",
      "     |      clf.fit(X_train, y_train,\n",
      "     |              eval_set=[(X_train, y_train), (X_test, y_test)],\n",
      "     |              eval_metric='logloss',\n",
      "     |              verbose=True)\n",
      "     |      \n",
      "     |      evals_result = clf.evals_result()\n",
      "     |      \n",
      "     |      The variable evals_result will contain:\n",
      "     |      {'validation_0': {'logloss': ['0.604835', '0.531479']},\n",
      "     |       'validation_1': {'logloss': ['0.41965', '0.17686']}}\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None, eval_set=None, eval_metric=None, early_stopping_rounds=None, verbose=True, xgb_model=None)\n",
      "     |      Fit gradient boosting classifier\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array_like\n",
      "     |          Feature matrix\n",
      "     |      y : array_like\n",
      "     |          Labels\n",
      "     |      sample_weight : array_like\n",
      "     |          Weight for each instance\n",
      "     |      eval_set : list, optional\n",
      "     |          A list of (X, y) pairs to use as a validation set for\n",
      "     |          early-stopping\n",
      "     |      eval_metric : str, callable, optional\n",
      "     |          If a str, should be a built-in evaluation metric to use. See\n",
      "     |          doc/parameter.md. If callable, a custom evaluation metric. The call\n",
      "     |          signature is func(y_predicted, y_true) where y_true will be a\n",
      "     |          DMatrix object such that you may need to call the get_label\n",
      "     |          method. It must return a str, value pair where the str is a name\n",
      "     |          for the evaluation and value is the value of the evaluation\n",
      "     |          function. This objective is always minimized.\n",
      "     |      early_stopping_rounds : int, optional\n",
      "     |          Activates early stopping. Validation error needs to decrease at\n",
      "     |          least every <early_stopping_rounds> round(s) to continue training.\n",
      "     |          Requires at least one item in evals.  If there's more than one,\n",
      "     |          will use the last. Returns the model from the last iteration\n",
      "     |          (not the best one). If early stopping occurs, the model will\n",
      "     |          have three additional fields: bst.best_score, bst.best_iteration\n",
      "     |          and bst.best_ntree_limit.\n",
      "     |          (Use bst.best_ntree_limit to get the correct value if num_parallel_tree\n",
      "     |          and/or num_class appears in the parameters)\n",
      "     |      verbose : bool\n",
      "     |          If `verbose` and an evaluation set is used, writes the evaluation\n",
      "     |          metric measured on the validation set to stderr.\n",
      "     |      xgb_model : str\n",
      "     |          file name of stored xgb model or 'Booster' instance Xgb model to be\n",
      "     |          loaded before training (allows training continuation).\n",
      "     |  \n",
      "     |  predict(self, data, output_margin=False, ntree_limit=0)\n",
      "     |  \n",
      "     |  predict_proba(self, data, output_margin=False, ntree_limit=0)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from XGBModel:\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  apply(self, X, ntree_limit=0)\n",
      "     |      Return the predicted leaf every tree for each sample.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array_like, shape=[n_samples, n_features]\n",
      "     |          Input features matrix.\n",
      "     |      \n",
      "     |      ntree_limit : int\n",
      "     |          Limit number of trees in the prediction; defaults to 0 (use all trees).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_leaves : array_like, shape=[n_samples, n_trees]\n",
      "     |          For each datapoint x in X and for each tree, return the index of the\n",
      "     |          leaf x ends up in. Leaves are numbered within\n",
      "     |          ``[0; 2**(self.max_depth+1))``, possibly with gaps in the numbering.\n",
      "     |  \n",
      "     |  get_booster(self)\n",
      "     |      Get the underlying xgboost Booster of this model.\n",
      "     |      \n",
      "     |      This will raise an exception when fit was not called\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      booster : a xgboost booster of underlying model\n",
      "     |  \n",
      "     |  get_params(self, deep=False)\n",
      "     |      Get parameters.\n",
      "     |  \n",
      "     |  get_xgb_params(self)\n",
      "     |      Get xgboost type parameters.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from XGBModel:\n",
      "     |  \n",
      "     |  feature_importances_\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      feature_importances_ : array of shape = [n_features]\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the mean accuracy on the given test data and labels.\n",
      "     |      \n",
      "     |      In multi-label classification, this is the subset accuracy\n",
      "     |      which is a harsh metric since you require for each sample that\n",
      "     |      each label set be correctly predicted.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True labels for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Mean accuracy of self.predict(X) wrt. y.\n",
      "    \n",
      "    class XGBModel(sklearn.base.BaseEstimator)\n",
      "     |  Implementation of the Scikit-Learn API for XGBoost.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  max_depth : int\n",
      "     |      Maximum tree depth for base learners.\n",
      "     |  learning_rate : float\n",
      "     |      Boosting learning rate (xgb's \"eta\")\n",
      "     |  n_estimators : int\n",
      "     |      Number of boosted trees to fit.\n",
      "     |  silent : boolean\n",
      "     |      Whether to print messages while running boosting.\n",
      "     |  objective : string or callable\n",
      "     |      Specify the learning task and the corresponding learning objective or\n",
      "     |      a custom objective function to be used (see note below).\n",
      "     |  booster: string\n",
      "     |      Specify which booster to use: gbtree, gblinear or dart.\n",
      "     |  nthread : int\n",
      "     |      Number of parallel threads used to run xgboost.  (Deprecated, please use n_jobs)\n",
      "     |  n_jobs : int\n",
      "     |      Number of parallel threads used to run xgboost.  (replaces nthread)\n",
      "     |  gamma : float\n",
      "     |      Minimum loss reduction required to make a further partition on a leaf node of the tree.\n",
      "     |  min_child_weight : int\n",
      "     |      Minimum sum of instance weight(hessian) needed in a child.\n",
      "     |  max_delta_step : int\n",
      "     |      Maximum delta step we allow each tree's weight estimation to be.\n",
      "     |  subsample : float\n",
      "     |      Subsample ratio of the training instance.\n",
      "     |  colsample_bytree : float\n",
      "     |      Subsample ratio of columns when constructing each tree.\n",
      "     |  colsample_bylevel : float\n",
      "     |      Subsample ratio of columns for each split, in each level.\n",
      "     |  reg_alpha : float (xgb's alpha)\n",
      "     |      L1 regularization term on weights\n",
      "     |  reg_lambda : float (xgb's lambda)\n",
      "     |      L2 regularization term on weights\n",
      "     |  scale_pos_weight : float\n",
      "     |      Balancing of positive and negative weights.\n",
      "     |  base_score:\n",
      "     |      The initial prediction score of all instances, global bias.\n",
      "     |  seed : int\n",
      "     |      Random number seed.  (Deprecated, please use random_state)\n",
      "     |  random_state : int\n",
      "     |      Random number seed.  (replaces seed)\n",
      "     |  missing : float, optional\n",
      "     |      Value in the data which needs to be present as a missing value. If\n",
      "     |      None, defaults to np.nan.\n",
      "     |  **kwargs : dict, optional\n",
      "     |      Keyword arguments for XGBoost Booster object.  Full documentation of parameters can\n",
      "     |      be found here: https://github.com/dmlc/xgboost/blob/master/doc/parameter.md.\n",
      "     |      Attempting to set a parameter via the constructor args and **kwargs dict simultaneously\n",
      "     |      will result in a TypeError.\n",
      "     |      Note:\n",
      "     |          **kwargs is unsupported by Sklearn.  We do not guarantee that parameters passed via\n",
      "     |          this argument will interact properly with Sklearn.\n",
      "     |  \n",
      "     |  Note\n",
      "     |  ----\n",
      "     |  A custom objective function can be provided for the ``objective``\n",
      "     |  parameter. In this case, it should have the signature\n",
      "     |  ``objective(y_true, y_pred) -> grad, hess``:\n",
      "     |  \n",
      "     |  y_true: array_like of shape [n_samples]\n",
      "     |      The target values\n",
      "     |  y_pred: array_like of shape [n_samples]\n",
      "     |      The predicted values\n",
      "     |  \n",
      "     |  grad: array_like of shape [n_samples]\n",
      "     |      The value of the gradient for each sample point.\n",
      "     |  hess: array_like of shape [n_samples]\n",
      "     |      The value of the second derivative for each sample point\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      XGBModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, max_depth=3, learning_rate=0.1, n_estimators=100, silent=True, objective='reg:linear', booster='gbtree', n_jobs=1, nthread=None, gamma=0, min_child_weight=1, max_delta_step=0, subsample=1, colsample_bytree=1, colsample_bylevel=1, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, base_score=0.5, random_state=0, seed=None, missing=None, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  apply(self, X, ntree_limit=0)\n",
      "     |      Return the predicted leaf every tree for each sample.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array_like, shape=[n_samples, n_features]\n",
      "     |          Input features matrix.\n",
      "     |      \n",
      "     |      ntree_limit : int\n",
      "     |          Limit number of trees in the prediction; defaults to 0 (use all trees).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_leaves : array_like, shape=[n_samples, n_trees]\n",
      "     |          For each datapoint x in X and for each tree, return the index of the\n",
      "     |          leaf x ends up in. Leaves are numbered within\n",
      "     |          ``[0; 2**(self.max_depth+1))``, possibly with gaps in the numbering.\n",
      "     |  \n",
      "     |  evals_result(self)\n",
      "     |      Return the evaluation results.\n",
      "     |      \n",
      "     |      If eval_set is passed to the `fit` function, you can call evals_result() to\n",
      "     |      get evaluation results for all passed eval_sets. When eval_metric is also\n",
      "     |      passed to the `fit` function, the evals_result will contain the eval_metrics\n",
      "     |      passed to the `fit` function\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      evals_result : dictionary\n",
      "     |      \n",
      "     |      Example\n",
      "     |      -------\n",
      "     |      param_dist = {'objective':'binary:logistic', 'n_estimators':2}\n",
      "     |      \n",
      "     |      clf = xgb.XGBModel(**param_dist)\n",
      "     |      \n",
      "     |      clf.fit(X_train, y_train,\n",
      "     |              eval_set=[(X_train, y_train), (X_test, y_test)],\n",
      "     |              eval_metric='logloss',\n",
      "     |              verbose=True)\n",
      "     |      \n",
      "     |      evals_result = clf.evals_result()\n",
      "     |      \n",
      "     |      The variable evals_result will contain:\n",
      "     |      {'validation_0': {'logloss': ['0.604835', '0.531479']},\n",
      "     |       'validation_1': {'logloss': ['0.41965', '0.17686']}}\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None, eval_set=None, eval_metric=None, early_stopping_rounds=None, verbose=True, xgb_model=None)\n",
      "     |      Fit the gradient boosting model\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array_like\n",
      "     |          Feature matrix\n",
      "     |      y : array_like\n",
      "     |          Labels\n",
      "     |      sample_weight : array_like\n",
      "     |          instance weights\n",
      "     |      eval_set : list, optional\n",
      "     |          A list of (X, y) tuple pairs to use as a validation set for\n",
      "     |          early-stopping\n",
      "     |      eval_metric : str, callable, optional\n",
      "     |          If a str, should be a built-in evaluation metric to use. See\n",
      "     |          doc/parameter.md. If callable, a custom evaluation metric. The call\n",
      "     |          signature is func(y_predicted, y_true) where y_true will be a\n",
      "     |          DMatrix object such that you may need to call the get_label\n",
      "     |          method. It must return a str, value pair where the str is a name\n",
      "     |          for the evaluation and value is the value of the evaluation\n",
      "     |          function. This objective is always minimized.\n",
      "     |      early_stopping_rounds : int\n",
      "     |          Activates early stopping. Validation error needs to decrease at\n",
      "     |          least every <early_stopping_rounds> round(s) to continue training.\n",
      "     |          Requires at least one item in evals.  If there's more than one,\n",
      "     |          will use the last. Returns the model from the last iteration\n",
      "     |          (not the best one). If early stopping occurs, the model will\n",
      "     |          have three additional fields: bst.best_score, bst.best_iteration\n",
      "     |          and bst.best_ntree_limit.\n",
      "     |          (Use bst.best_ntree_limit to get the correct value if num_parallel_tree\n",
      "     |          and/or num_class appears in the parameters)\n",
      "     |      verbose : bool\n",
      "     |          If `verbose` and an evaluation set is used, writes the evaluation\n",
      "     |          metric measured on the validation set to stderr.\n",
      "     |      xgb_model : str\n",
      "     |          file name of stored xgb model or 'Booster' instance Xgb model to be\n",
      "     |          loaded before training (allows training continuation).\n",
      "     |  \n",
      "     |  get_booster(self)\n",
      "     |      Get the underlying xgboost Booster of this model.\n",
      "     |      \n",
      "     |      This will raise an exception when fit was not called\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      booster : a xgboost booster of underlying model\n",
      "     |  \n",
      "     |  get_params(self, deep=False)\n",
      "     |      Get parameters.\n",
      "     |  \n",
      "     |  get_xgb_params(self)\n",
      "     |      Get xgboost type parameters.\n",
      "     |  \n",
      "     |  predict(self, data, output_margin=False, ntree_limit=0)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  feature_importances_\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      feature_importances_ : array of shape = [n_features]\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class XGBRegressor(XGBModel, sklearn.base.RegressorMixin)\n",
      "     |  Implementation of the scikit-learn API for XGBoost regression.\n",
      "     |      Parameters\n",
      "     |  ----------\n",
      "     |  max_depth : int\n",
      "     |      Maximum tree depth for base learners.\n",
      "     |  learning_rate : float\n",
      "     |      Boosting learning rate (xgb's \"eta\")\n",
      "     |  n_estimators : int\n",
      "     |      Number of boosted trees to fit.\n",
      "     |  silent : boolean\n",
      "     |      Whether to print messages while running boosting.\n",
      "     |  objective : string or callable\n",
      "     |      Specify the learning task and the corresponding learning objective or\n",
      "     |      a custom objective function to be used (see note below).\n",
      "     |  booster: string\n",
      "     |      Specify which booster to use: gbtree, gblinear or dart.\n",
      "     |  nthread : int\n",
      "     |      Number of parallel threads used to run xgboost.  (Deprecated, please use n_jobs)\n",
      "     |  n_jobs : int\n",
      "     |      Number of parallel threads used to run xgboost.  (replaces nthread)\n",
      "     |  gamma : float\n",
      "     |      Minimum loss reduction required to make a further partition on a leaf node of the tree.\n",
      "     |  min_child_weight : int\n",
      "     |      Minimum sum of instance weight(hessian) needed in a child.\n",
      "     |  max_delta_step : int\n",
      "     |      Maximum delta step we allow each tree's weight estimation to be.\n",
      "     |  subsample : float\n",
      "     |      Subsample ratio of the training instance.\n",
      "     |  colsample_bytree : float\n",
      "     |      Subsample ratio of columns when constructing each tree.\n",
      "     |  colsample_bylevel : float\n",
      "     |      Subsample ratio of columns for each split, in each level.\n",
      "     |  reg_alpha : float (xgb's alpha)\n",
      "     |      L1 regularization term on weights\n",
      "     |  reg_lambda : float (xgb's lambda)\n",
      "     |      L2 regularization term on weights\n",
      "     |  scale_pos_weight : float\n",
      "     |      Balancing of positive and negative weights.\n",
      "     |  base_score:\n",
      "     |      The initial prediction score of all instances, global bias.\n",
      "     |  seed : int\n",
      "     |      Random number seed.  (Deprecated, please use random_state)\n",
      "     |  random_state : int\n",
      "     |      Random number seed.  (replaces seed)\n",
      "     |  missing : float, optional\n",
      "     |      Value in the data which needs to be present as a missing value. If\n",
      "     |      None, defaults to np.nan.\n",
      "     |  **kwargs : dict, optional\n",
      "     |      Keyword arguments for XGBoost Booster object.  Full documentation of parameters can\n",
      "     |      be found here: https://github.com/dmlc/xgboost/blob/master/doc/parameter.md.\n",
      "     |      Attempting to set a parameter via the constructor args and **kwargs dict simultaneously\n",
      "     |      will result in a TypeError.\n",
      "     |      Note:\n",
      "     |          **kwargs is unsupported by Sklearn.  We do not guarantee that parameters passed via\n",
      "     |          this argument will interact properly with Sklearn.\n",
      "     |  \n",
      "     |  Note\n",
      "     |  ----\n",
      "     |  A custom objective function can be provided for the ``objective``\n",
      "     |  parameter. In this case, it should have the signature\n",
      "     |  ``objective(y_true, y_pred) -> grad, hess``:\n",
      "     |  \n",
      "     |  y_true: array_like of shape [n_samples]\n",
      "     |      The target values\n",
      "     |  y_pred: array_like of shape [n_samples]\n",
      "     |      The predicted values\n",
      "     |  \n",
      "     |  grad: array_like of shape [n_samples]\n",
      "     |      The value of the gradient for each sample point.\n",
      "     |  hess: array_like of shape [n_samples]\n",
      "     |      The value of the second derivative for each sample point\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      XGBRegressor\n",
      "     |      XGBModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods inherited from XGBModel:\n",
      "     |  \n",
      "     |  __init__(self, max_depth=3, learning_rate=0.1, n_estimators=100, silent=True, objective='reg:linear', booster='gbtree', n_jobs=1, nthread=None, gamma=0, min_child_weight=1, max_delta_step=0, subsample=1, colsample_bytree=1, colsample_bylevel=1, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, base_score=0.5, random_state=0, seed=None, missing=None, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  apply(self, X, ntree_limit=0)\n",
      "     |      Return the predicted leaf every tree for each sample.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array_like, shape=[n_samples, n_features]\n",
      "     |          Input features matrix.\n",
      "     |      \n",
      "     |      ntree_limit : int\n",
      "     |          Limit number of trees in the prediction; defaults to 0 (use all trees).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_leaves : array_like, shape=[n_samples, n_trees]\n",
      "     |          For each datapoint x in X and for each tree, return the index of the\n",
      "     |          leaf x ends up in. Leaves are numbered within\n",
      "     |          ``[0; 2**(self.max_depth+1))``, possibly with gaps in the numbering.\n",
      "     |  \n",
      "     |  evals_result(self)\n",
      "     |      Return the evaluation results.\n",
      "     |      \n",
      "     |      If eval_set is passed to the `fit` function, you can call evals_result() to\n",
      "     |      get evaluation results for all passed eval_sets. When eval_metric is also\n",
      "     |      passed to the `fit` function, the evals_result will contain the eval_metrics\n",
      "     |      passed to the `fit` function\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      evals_result : dictionary\n",
      "     |      \n",
      "     |      Example\n",
      "     |      -------\n",
      "     |      param_dist = {'objective':'binary:logistic', 'n_estimators':2}\n",
      "     |      \n",
      "     |      clf = xgb.XGBModel(**param_dist)\n",
      "     |      \n",
      "     |      clf.fit(X_train, y_train,\n",
      "     |              eval_set=[(X_train, y_train), (X_test, y_test)],\n",
      "     |              eval_metric='logloss',\n",
      "     |              verbose=True)\n",
      "     |      \n",
      "     |      evals_result = clf.evals_result()\n",
      "     |      \n",
      "     |      The variable evals_result will contain:\n",
      "     |      {'validation_0': {'logloss': ['0.604835', '0.531479']},\n",
      "     |       'validation_1': {'logloss': ['0.41965', '0.17686']}}\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None, eval_set=None, eval_metric=None, early_stopping_rounds=None, verbose=True, xgb_model=None)\n",
      "     |      Fit the gradient boosting model\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array_like\n",
      "     |          Feature matrix\n",
      "     |      y : array_like\n",
      "     |          Labels\n",
      "     |      sample_weight : array_like\n",
      "     |          instance weights\n",
      "     |      eval_set : list, optional\n",
      "     |          A list of (X, y) tuple pairs to use as a validation set for\n",
      "     |          early-stopping\n",
      "     |      eval_metric : str, callable, optional\n",
      "     |          If a str, should be a built-in evaluation metric to use. See\n",
      "     |          doc/parameter.md. If callable, a custom evaluation metric. The call\n",
      "     |          signature is func(y_predicted, y_true) where y_true will be a\n",
      "     |          DMatrix object such that you may need to call the get_label\n",
      "     |          method. It must return a str, value pair where the str is a name\n",
      "     |          for the evaluation and value is the value of the evaluation\n",
      "     |          function. This objective is always minimized.\n",
      "     |      early_stopping_rounds : int\n",
      "     |          Activates early stopping. Validation error needs to decrease at\n",
      "     |          least every <early_stopping_rounds> round(s) to continue training.\n",
      "     |          Requires at least one item in evals.  If there's more than one,\n",
      "     |          will use the last. Returns the model from the last iteration\n",
      "     |          (not the best one). If early stopping occurs, the model will\n",
      "     |          have three additional fields: bst.best_score, bst.best_iteration\n",
      "     |          and bst.best_ntree_limit.\n",
      "     |          (Use bst.best_ntree_limit to get the correct value if num_parallel_tree\n",
      "     |          and/or num_class appears in the parameters)\n",
      "     |      verbose : bool\n",
      "     |          If `verbose` and an evaluation set is used, writes the evaluation\n",
      "     |          metric measured on the validation set to stderr.\n",
      "     |      xgb_model : str\n",
      "     |          file name of stored xgb model or 'Booster' instance Xgb model to be\n",
      "     |          loaded before training (allows training continuation).\n",
      "     |  \n",
      "     |  get_booster(self)\n",
      "     |      Get the underlying xgboost Booster of this model.\n",
      "     |      \n",
      "     |      This will raise an exception when fit was not called\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      booster : a xgboost booster of underlying model\n",
      "     |  \n",
      "     |  get_params(self, deep=False)\n",
      "     |      Get parameters.\n",
      "     |  \n",
      "     |  get_xgb_params(self)\n",
      "     |      Get xgboost type parameters.\n",
      "     |  \n",
      "     |  predict(self, data, output_margin=False, ntree_limit=0)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from XGBModel:\n",
      "     |  \n",
      "     |  feature_importances_\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      feature_importances_ : array of shape = [n_features]\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the coefficient of determination R^2 of the prediction.\n",
      "     |      \n",
      "     |      The coefficient R^2 is defined as (1 - u/v), where u is the residual\n",
      "     |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n",
      "     |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always\n",
      "     |      predicts the expected value of y, disregarding the input features,\n",
      "     |      would get a R^2 score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True values for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          R^2 of self.predict(X) wrt. y.\n",
      "\n",
      "FUNCTIONS\n",
      "    cv(params, dtrain, num_boost_round=10, nfold=3, stratified=False, folds=None, metrics=(), obj=None, feval=None, maximize=False, early_stopping_rounds=None, fpreproc=None, as_pandas=True, verbose_eval=None, show_stdv=True, seed=0, callbacks=None, shuffle=True)\n",
      "        Cross-validation with given parameters.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        params : dict\n",
      "            Booster params.\n",
      "        dtrain : DMatrix\n",
      "            Data to be trained.\n",
      "        num_boost_round : int\n",
      "            Number of boosting iterations.\n",
      "        nfold : int\n",
      "            Number of folds in CV.\n",
      "        stratified : bool\n",
      "            Perform stratified sampling.\n",
      "        folds : a KFold or StratifiedKFold instance\n",
      "            Sklearn KFolds or StratifiedKFolds.\n",
      "        metrics : string or list of strings\n",
      "            Evaluation metrics to be watched in CV.\n",
      "        obj : function\n",
      "            Custom objective function.\n",
      "        feval : function\n",
      "            Custom evaluation function.\n",
      "        maximize : bool\n",
      "            Whether to maximize feval.\n",
      "        early_stopping_rounds: int\n",
      "            Activates early stopping. CV error needs to decrease at least\n",
      "            every <early_stopping_rounds> round(s) to continue.\n",
      "            Last entry in evaluation history is the one from best iteration.\n",
      "        fpreproc : function\n",
      "            Preprocessing function that takes (dtrain, dtest, param) and returns\n",
      "            transformed versions of those.\n",
      "        as_pandas : bool, default True\n",
      "            Return pd.DataFrame when pandas is installed.\n",
      "            If False or pandas is not installed, return np.ndarray\n",
      "        verbose_eval : bool, int, or None, default None\n",
      "            Whether to display the progress. If None, progress will be displayed\n",
      "            when np.ndarray is returned. If True, progress will be displayed at\n",
      "            boosting stage. If an integer is given, progress will be displayed\n",
      "            at every given `verbose_eval` boosting stage.\n",
      "        show_stdv : bool, default True\n",
      "            Whether to display the standard deviation in progress.\n",
      "            Results are not affected, and always contains std.\n",
      "        seed : int\n",
      "            Seed used to generate the folds (passed to numpy.random.seed).\n",
      "        callbacks : list of callback functions\n",
      "            List of callback functions that are applied at end of each iteration.\n",
      "            It is possible to use predefined callbacks by using xgb.callback module.\n",
      "            Example: [xgb.callback.reset_learning_rate(custom_rates)]\n",
      "         shuffle : bool\n",
      "            Shuffle data before creating folds.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        evaluation history : list(string)\n",
      "    \n",
      "    plot_importance(booster, ax=None, height=0.2, xlim=None, ylim=None, title='Feature importance', xlabel='F score', ylabel='Features', importance_type='weight', max_num_features=None, grid=True, show_values=True, **kwargs)\n",
      "        Plot importance based on fitted trees.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        booster : Booster, XGBModel or dict\n",
      "            Booster or XGBModel instance, or dict taken by Booster.get_fscore()\n",
      "        ax : matplotlib Axes, default None\n",
      "            Target axes instance. If None, new figure and axes will be created.\n",
      "        grid : bool, Turn the axes grids on or off.  Default is True (On).\n",
      "        importance_type : str, default \"weight\"\n",
      "            How the importance is calculated: either \"weight\", \"gain\", or \"cover\"\n",
      "            \"weight\" is the number of times a feature appears in a tree\n",
      "            \"gain\" is the average gain of splits which use the feature\n",
      "            \"cover\" is the average coverage of splits which use the feature\n",
      "                where coverage is defined as the number of samples affected by the split\n",
      "        max_num_features : int, default None\n",
      "            Maximum number of top features displayed on plot. If None, all features will be displayed.\n",
      "        height : float, default 0.2\n",
      "            Bar height, passed to ax.barh()\n",
      "        xlim : tuple, default None\n",
      "            Tuple passed to axes.xlim()\n",
      "        ylim : tuple, default None\n",
      "            Tuple passed to axes.ylim()\n",
      "        title : str, default \"Feature importance\"\n",
      "            Axes title. To disable, pass None.\n",
      "        xlabel : str, default \"F score\"\n",
      "            X axis title label. To disable, pass None.\n",
      "        ylabel : str, default \"Features\"\n",
      "            Y axis title label. To disable, pass None.\n",
      "        show_values : bool, default True\n",
      "            Show values on plot. To disable, pass False.\n",
      "        kwargs :\n",
      "            Other keywords passed to ax.barh()\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        ax : matplotlib Axes\n",
      "    \n",
      "    plot_tree(booster, fmap='', num_trees=0, rankdir='UT', ax=None, **kwargs)\n",
      "        Plot specified tree.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        booster : Booster, XGBModel\n",
      "            Booster or XGBModel instance\n",
      "        fmap: str (optional)\n",
      "           The name of feature map file\n",
      "        num_trees : int, default 0\n",
      "            Specify the ordinal number of target tree\n",
      "        rankdir : str, default \"UT\"\n",
      "            Passed to graphiz via graph_attr\n",
      "        ax : matplotlib Axes, default None\n",
      "            Target axes instance. If None, new figure and axes will be created.\n",
      "        kwargs :\n",
      "            Other keywords passed to to_graphviz\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        ax : matplotlib Axes\n",
      "    \n",
      "    to_graphviz(booster, fmap='', num_trees=0, rankdir='UT', yes_color='#0000FF', no_color='#FF0000', **kwargs)\n",
      "        Convert specified tree to graphviz instance. IPython can automatically plot the\n",
      "        returned graphiz instance. Otherwise, you should call .render() method\n",
      "        of the returned graphiz instance.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        booster : Booster, XGBModel\n",
      "            Booster or XGBModel instance\n",
      "        fmap: str (optional)\n",
      "           The name of feature map file\n",
      "        num_trees : int, default 0\n",
      "            Specify the ordinal number of target tree\n",
      "        rankdir : str, default \"UT\"\n",
      "            Passed to graphiz via graph_attr\n",
      "        yes_color : str, default '#0000FF'\n",
      "            Edge color when meets the node condition.\n",
      "        no_color : str, default '#FF0000'\n",
      "            Edge color when doesn't meet the node condition.\n",
      "        kwargs :\n",
      "            Other keywords passed to graphviz graph_attr\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        ax : matplotlib Axes\n",
      "    \n",
      "    train(params, dtrain, num_boost_round=10, evals=(), obj=None, feval=None, maximize=False, early_stopping_rounds=None, evals_result=None, verbose_eval=True, xgb_model=None, callbacks=None, learning_rates=None)\n",
      "        Train a booster with given parameters.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        params : dict\n",
      "            Booster params.\n",
      "        dtrain : DMatrix\n",
      "            Data to be trained.\n",
      "        num_boost_round: int\n",
      "            Number of boosting iterations.\n",
      "        evals: list of pairs (DMatrix, string)\n",
      "            List of items to be evaluated during training, this allows user to watch\n",
      "            performance on the validation set.\n",
      "        obj : function\n",
      "            Customized objective function.\n",
      "        feval : function\n",
      "            Customized evaluation function.\n",
      "        maximize : bool\n",
      "            Whether to maximize feval.\n",
      "        early_stopping_rounds: int\n",
      "            Activates early stopping. Validation error needs to decrease at least\n",
      "            every <early_stopping_rounds> round(s) to continue training.\n",
      "            Requires at least one item in evals.\n",
      "            If there's more than one, will use the last.\n",
      "            Returns the model from the last iteration (not the best one).\n",
      "            If early stopping occurs, the model will have three additional fields:\n",
      "            bst.best_score, bst.best_iteration and bst.best_ntree_limit.\n",
      "            (Use bst.best_ntree_limit to get the correct value if num_parallel_tree\n",
      "            and/or num_class appears in the parameters)\n",
      "        evals_result: dict\n",
      "            This dictionary stores the evaluation results of all the items in watchlist.\n",
      "            Example: with a watchlist containing [(dtest,'eval'), (dtrain,'train')] and\n",
      "            a parameter containing ('eval_metric': 'logloss')\n",
      "            Returns: {'train': {'logloss': ['0.48253', '0.35953']},\n",
      "                      'eval': {'logloss': ['0.480385', '0.357756']}}\n",
      "        verbose_eval : bool or int\n",
      "            Requires at least one item in evals.\n",
      "            If `verbose_eval` is True then the evaluation metric on the validation set is\n",
      "            printed at each boosting stage.\n",
      "            If `verbose_eval` is an integer then the evaluation metric on the validation set\n",
      "            is printed at every given `verbose_eval` boosting stage. The last boosting stage\n",
      "            / the boosting stage found by using `early_stopping_rounds` is also printed.\n",
      "            Example: with verbose_eval=4 and at least one item in evals, an evaluation metric\n",
      "            is printed every 4 boosting stages, instead of every boosting stage.\n",
      "        learning_rates: list or function (deprecated - use callback API instead)\n",
      "            List of learning rate for each boosting round\n",
      "            or a customized function that calculates eta in terms of\n",
      "            current number of round and the total number of boosting round (e.g. yields\n",
      "            learning rate decay)\n",
      "        xgb_model : file name of stored xgb model or 'Booster' instance\n",
      "            Xgb model to be loaded before training (allows training continuation).\n",
      "        callbacks : list of callback functions\n",
      "            List of callback functions that are applied at end of each iteration.\n",
      "            It is possible to use predefined callbacks by using xgb.callback module.\n",
      "            Example: [xgb.callback.reset_learning_rate(custom_rates)]\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        booster : a trained booster model\n",
      "\n",
      "DATA\n",
      "    __all__ = ['DMatrix', 'Booster', 'train', 'cv', 'XGBModel', 'XGBClassi...\n",
      "\n",
      "VERSION\n",
      "    0.7\n",
      "\n",
      "FILE\n",
      "    c:\\users\\cfabbri\\anaconda3\\lib\\site-packages\\xgboost\\__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(xgboost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
